{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent and Dataset Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import random, math\n",
    "import import_ipynb\n",
    "from stats_and_prob import *\n",
    "from linear_algebra import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a few basic tools to help with data tasks (vector manipulation and the like), gradient descent algorithms, and PCA routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(A):\n",
    "    num_rows = len(A)\n",
    "    num_cols = len(A[0]) if A else 0\n",
    "    return num_rows, num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_multiply(c, v):\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_subtract(v, w):\n",
    "    '''componentwise subtraction of two vectors'''\n",
    "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def dot(v, w):\n",
    "    '''v_1 * w_1 + ... + v_n * w_n'''\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    '''v_1 * v_1 + ... + v_n * v_n'''\n",
    "    return dot(v, v)\n",
    "\n",
    "def squared_distance(v, w):\n",
    "    return sum_of_squares(vector_subtract(v, w))\n",
    "\n",
    "def distance(v, w):\n",
    "   return math.sqrt(squared_distance(v, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f, v, i, h):\n",
    "    '''computes the ith partial difference quotient of f at v'''\n",
    "    w = [v_j + (h if j == i else 0) for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v))/h\n",
    "\n",
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    grad = [partial_difference_quotient(f, v, i, h) for i, _ in enumerate(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return [v_i + step_size * direction_i\n",
    "            for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "def safe(f):\n",
    "    \"\"\"define a new function that wraps f and return it\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')         # this means \"infinity\" in Python\n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent plays many roles in data science, but most notably it serves as a way to find the minimum or maximum of functions when fitting models.  The general idea is to calculate the gradient and move either with it or against it.  For instance, when finding the best fit of a model where we seek to minimize the sum of squares, the algorithm would step through the parameter space in a way that goes against the gradient to find the parameters that minimize the error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\n",
    "\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "    theta = theta_0                           # set theta to initial value\n",
    "    target_fn = safe(target_fn)               # safe version of target_fn\n",
    "    value = target_fn(theta)                  # value we're minimizing\n",
    "\n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                       for step_size in step_sizes]\n",
    "\n",
    "        # choose the one that minimizes the error function\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "\n",
    "        # stop if we're \"converging\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate(f):\n",
    "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                          negate_all(gradient_fn),\n",
    "                          theta_0,\n",
    "                          tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than using a batch approach of calculating error functions for the entire dataset, which can be computationally expensive, we can use stochastic gradient descent (SGD) when errors are additive.  Instead, we can start updating the model parameters from the first sample of the training data (chosen randomly) to stochastically converge on the optimal parameters.  Formally, this only serves to approximate the best fit, but it is much faster and for practical purposes just as adequate as full gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_random_order(data):\n",
    "    '''generator that returns the elements of dataset in random order'''\n",
    "    indexes = [i for i, _ in enumerate(data)]    # create a list of indexes\n",
    "    random.shuffle(indexes)                      # shuffle in place\n",
    "    for i in indexes:\n",
    "        yield data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    \n",
    "    data = list(zip(x, y))\n",
    "    theta = theta_0                             # initial guess\n",
    "    alpha = alpha_0                             # initial step size\n",
    "    min_theta, min_value = None, float('inf')   # the minimum so far\n",
    "    iterations_with_no_improvement = 0\n",
    "    \n",
    "    # if we go more than 100 iterations without improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum(target_fn(x_i, y_i, theta) for x_i, y_i in data)\n",
    "        \n",
    "        if value < min_value:\n",
    "            # if we find a new minimum, remember it\n",
    "            # and go back to original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # otherwise we're not improving, so try shrinking the step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "        \n",
    "        # and take a gradient step for each of the data points\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "            \n",
    "    return min_theta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimize_stochastic(negate(target_fn),\n",
    "                               negate_all(gradient_fn),\n",
    "                               x, y, theta_0, alpha_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(data):\n",
    "    '''returns the num_columns x num_columns matrix whose (i,j)th \n",
    "    entry is the correlation between columns i and j of the data'''\n",
    "    _, num_columns = shape(data)\n",
    "    def matrix_entry(i, j):\n",
    "        return correlation(get_column(data, i), get_column(data, j))\n",
    "    \n",
    "    return make_matrix(num_columns, num_columns, matrix_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(data_matrix):\n",
    "    '''returns the means and standard deviations of each column'''\n",
    "    num_rows, num_cols = shape(data_matrix)\n",
    "    means = [mean(get_column(data_matrix, j)) for j in range(num_cols)]\n",
    "    stdevs = [standard_deviation(get_column(data_matrix, j)) for j in range(num_cols)]\n",
    "    \n",
    "    return means, stdevs\n",
    "\n",
    "def rescale(data_matrix):\n",
    "    '''rescales the input data so column has mean of 0 and stdev of 1'''\n",
    "    means, stdevs = scale(data_matrix)\n",
    "    \n",
    "    def rescaled(i, j):\n",
    "        if stdevs[j] > 0:\n",
    "            return (data_matrix[i][j]-means[j])/stdevs[j]\n",
    "        else:\n",
    "            return data_matrix[i][j]\n",
    "        \n",
    "    num_rows, num_cols = shape(data_matrix)\n",
    "    \n",
    "    return make_matrix(num_rows, num_cols, rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_mean_matrix(A):\n",
    "    '''returns the result of subtracting from every value \n",
    "    in A the mean value of its column'''\n",
    "    nr, nc = shape(A)\n",
    "    column_means, _ = scale(A)\n",
    "    return make_matrix(nr, nc, lambda i, j: A[i][j]-column_means[j])\n",
    "\n",
    "def direction(w):\n",
    "    mag = magnitude(w)\n",
    "    return [w_i/mag for w_i in w]\n",
    "\n",
    "def directional_variance_i(x_i, w):\n",
    "    '''the variance of the row x_i in the direction of w'''\n",
    "    return dot(x_i, direction(w))**2\n",
    "\n",
    "def directional_variance(X, w):\n",
    "    '''the variance of the data in the direction determined by w'''\n",
    "    return sum(directional_variance_i(x_i, w) for x_i in X)\n",
    "\n",
    "def directional_variance_gradient_i(x_i, w):\n",
    "    '''the contribution of row x_i to the gradient of the dirctoin-w variance'''\n",
    "    projection_length = dot(x_i, direction(w))\n",
    "    return [2*projection_length*x_ij for x_ij in x_i]\n",
    "\n",
    "def directional_variance_gradient(X, w):\n",
    "    return vector_sum(directional_variance_gradient_i(x_i, w) for x_i in X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle component analysis (PCA) is a way to reduce the dimensionality of your data by transforming it through projects and removing dimensions that contain less information.  The principle components are identified as the directions with the highest variance, i.e. the eigenvectors with the largest eigenvalues.  This technique is also useful because it reprojects your data into orthogonal directions.  For instance, consider a three-dimensional dataset that looks like an angled cigar.  The principle component is the long direction and PCA would re-orient the data through linear transformation to deproject it along the x, y, and z directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_principle_component(X):\n",
    "    guess = [1 for _ in X[0]]\n",
    "    unscaled_maximizer = maximize_batch(\n",
    "        partial(directional_variance, X),           # as a function of w\n",
    "        partial(directional_variance_gradient, X),  # as a function of w\n",
    "        guess)\n",
    "    return direction(unscaled_maximizer)\n",
    "\n",
    "def first_principle_component_sgd(X):\n",
    "    guess = [1 for _ in X[0]]\n",
    "    unscaled_maximizer = maximize_stochastic(\n",
    "        lambda x, _, w: directional_variance_i(x, w),\n",
    "        lambda x, _, w: directional_variance_gradient_i(x, w),\n",
    "        X,\n",
    "        [None for _ in X],\n",
    "        guess)\n",
    "    return direction(unscaled_maximizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(v, w):\n",
    "    '''return the projection of v onto the direction w'''\n",
    "    projection_length = dot(v, w)\n",
    "    return scalar_multiply(projection_length, w)\n",
    "\n",
    "def remove_projection_from_vector(v, w):\n",
    "    '''projects v onto w and subtracts the result from v'''\n",
    "    return vector_subtract(v, project(v,w))\n",
    "\n",
    "def remove_projection(X,w):\n",
    "    '''for each row of X projects the row onto w, \n",
    "    then subtracts result from row'''\n",
    "    return [remove_projection_from_vector(x_i, w) for x_i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def principal_component_analysis(X, num_components):\n",
    "    components=[]\n",
    "    for _ in range(num_components):\n",
    "        component = first_principal_component(X)\n",
    "        components.append(component)\n",
    "        X = remove_projection(X, component)\n",
    "    \n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_vector(v, components):\n",
    "    return [dot(v, w) for w in components]\n",
    "\n",
    "def transform(X, components):\n",
    "    return [transform_vector(x_i, components) for x_i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
